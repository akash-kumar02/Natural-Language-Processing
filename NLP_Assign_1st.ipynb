{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment 1:**\n",
        " Perform tokenization, stopword removal, stemming, and lemmatization on a sample dataset. Compare how these preprocessing steps impact the quality of text representation.\n",
        "\n",
        " **By Akash Kumar,**"
      ],
      "metadata": {
        "id": "iCWjNamBqcj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Required Libraries**"
      ],
      "metadata": {
        "id": "qJga0YoZqs2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQRuzidgpIlp",
        "outputId": "009834ba-4b0b-4515-aa7b-9050b3b8ed17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt-F9NM1o1ce"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Natural Language Processing (NLP) is one of the most important areas of Artificial Intelligence.\n",
        "It helps machines understand, interpret, and generate human language...\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "Tokenization splits text into individual words (tokens)."
      ],
      "metadata": {
        "id": "heWCDo7ppqIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4bp3p-9pYrm",
        "outputId": "cd1b1862-9982-48a7-bba7-5596738ed6c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'one', 'of', 'the', 'most', 'important', 'areas', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'machines', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword Removal**\n",
        "\n",
        "Stopwords are common words that add little meaning (e.g., is, are, the)."
      ],
      "metadata": {
        "id": "70bZas6ipsUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TspH-XTpZsF",
        "outputId": "23eb3a48-65ec-46de-eb43-da68dd63216a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'one', 'important', 'areas', 'Artificial', 'Intelligence', '.', 'helps', 'machines', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "Stemming reduces words to their root form (may not be a real word)."
      ],
      "metadata": {
        "id": "iVq_tq8Op34Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bL8m_E6p0Fd",
        "outputId": "df59cf07-06f6-4b6a-bdc8-46debec76ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'one', 'import', 'area', 'artifici', 'intellig', '.', 'help', 'machin', 'understand', ',', 'interpret', ',', 'gener', 'human', 'languag', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**\n",
        "\n",
        "Lemmatization converts words into meaningful base forms using vocabulary."
      ],
      "metadata": {
        "id": "V0OKD8mGqRY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6zX9yB2qOnu",
        "outputId": "a32e21cb-0c8d-4f14-de8a-37b2d9536778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'one', 'important', 'area', 'Artificial', 'Intelligence', '.', 'help', 'machine', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison of Text Preprocessing**\n",
        "\n",
        "**Tokenization:**\n",
        "\n",
        "Breaks long text into smaller words (tokens).\n",
        "\n",
        "✔ Makes text readable for machines\n",
        "\n",
        "✘ Does not reduce noise by itself\n",
        "\n",
        "**Stopword Removal:**\n",
        "\n",
        "Removes common words like is, the, and.\n",
        "\n",
        "✔ Reduces unnecessary words\n",
        "\n",
        "✘ May remove useful meaning in some cases\n",
        "\n",
        "**Stemming:**\n",
        "\n",
        "Cuts words to their root form (e.g., playing → play).\n",
        "\n",
        "✔ Reduces vocabulary size\n",
        "\n",
        "✘ Root words may not be grammatically correct\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        "Converts words to meaningful base form (e.g., better → good).\n",
        "\n",
        "✔ Preserves correct meaning\n",
        "\n",
        "✘ Slightly slower than stemming"
      ],
      "metadata": {
        "id": "jllTa3vvrmB2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jEbc5vG6qWLB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}